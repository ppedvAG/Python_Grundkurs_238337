{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c52acda-5ab1-49b3-9022-9f2cce74ddd8",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884feb6-8353-48a2-9524-d992a7414330",
   "metadata": {},
   "source": [
    "Im Gegensatz zum konventionellen Machine Learning gibt es beim Reinforcement Learning keine vorgegebenen Daten. Die Daten für das Trainieren des Models werden während des Prozesses generiert.\n",
    "\n",
    "Begriffe:\n",
    "\n",
    "- Umgebung\n",
    "    - Erzeugt kontinuierlich Trainingsdaten und gibt diese an das Model zurück\n",
    "    - Effektiv das Programm, welches trainiert werden soll\n",
    " \n",
    "- Prozess\n",
    "    - Schleife die das Programm ausführt\n",
    "    - Nimmt die Daten der Umgebung und gibt diese an den Agenten weiter\n",
    "\n",
    "- Agent\n",
    "    - Kennt die Regeln des Lernens (wie bekommen wir die Belohnung?)\n",
    "    - Führt die Actions aus (Beeinflussen der Umgebung)\n",
    "    - Die Umgebung gibt Punkte als Belohnung, je nach dem ob die Action positiv oder negativ war\n",
    " \n",
    "Packages:\n",
    "\n",
    "- Gym(nasium)\n",
    "    - Stellt Umgebung und Spaces bereit\n",
    "\n",
    "- Stable Baselines 3\n",
    "    - Stellt viele Extras zur Verfügung die beim Reinforcement hilfreich sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4bd2eeb-3cf2-4ea1-9387-52226e31f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5bb45-6506-454a-a326-74710efba4c1",
   "metadata": {},
   "source": [
    "## Vordefinierte Umgebungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb242c-89d0-4a78-b56f-28f9b64a26bb",
   "metadata": {},
   "source": [
    "Von OpenAI gibt es einige vordefinierte Umgebungen\n",
    "\n",
    "Eine davon ist die \"CartPole\" Umgebung\n",
    "\n",
    "Bei dieser geht es darum, eine Stange die auf einem Kart montiert ist möglichst gerade zu halten\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a47919ef-70aa-46d2-a601-f2126e6ae7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3d810-010c-42f3-b943-a44eb03c47d4",
   "metadata": {},
   "source": [
    "## Inhalte der Umgebung\n",
    "\n",
    "Zwei Oberbegriffe: Spaces, Funktionen\n",
    "\n",
    "Spaces:\n",
    "- Observation Space: Die Daten die beobachtet werden (der State nach einem Step)\n",
    "- Action Space: Die möglichen Aktionen des Agenten (z.B. Links, Rechts)\n",
    "- Box, Discrete, Tuple, Dict, ...\n",
    "\n",
    "Funktionen:\n",
    "- step(): Macht einen Schritt, berechnet die Belohnung anhand des States\n",
    "- reset(): Setzt die Umgebung für einen neuen Versuch\n",
    "- render(): Zeichnet die UI (falls vorhanden)\n",
    "- close(): Wird am Ende der Umgebung aufgerufen, wenn noch Dinge abgebaut werden müssen (z.B. UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8bfeb3-108a-4228-b7e1-fab000a9a85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchgang 0: Score: 11.0\n",
      "Durchgang 1: Score: 15.0\n",
      "Durchgang 2: Score: 11.0\n",
      "Durchgang 3: Score: 23.0\n",
      "Durchgang 4: Score: 14.0\n"
     ]
    }
   ],
   "source": [
    "durchgaenge = 5\n",
    "for x in range(durchgaenge):\n",
    "    state = env.reset()  # Gibt den Startstate zurück\n",
    "    score = 0  # Score sollte 200 erreichen\n",
    "\n",
    "    while True:\n",
    "        # env.render()\n",
    "        action = env.action_space.sample()  # Random Action auswählen\n",
    "        new_state, reward, done, term, info = env.step(action)  # Action an die Umgebung weitergeben\n",
    "        score += reward\n",
    "        if done or term:\n",
    "            break\n",
    "\n",
    "    print(f\"Durchgang {x}: Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8d0b3f2-9f02-4847-96cb-6d32ccffa22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8d770ea-3fd1-44bc-aca8-54f8c56cafd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f28016-b68b-49a9-b9b7-ff2be86e2d5e",
   "metadata": {},
   "source": [
    "## RL Modell Trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682aa98-d6f4-4da5-93e4-a41a5ca03a5c",
   "metadata": {},
   "source": [
    "Für ein RL Modell brauchen wir wieder ein paar Dinge:\n",
    "\n",
    "- Trainingsalgorithmus\n",
    "    - PPO: Proximal Policy Optimization\n",
    "    - https://stable-baselines.readthedocs.io/en/master/guide/algos.html\n",
    " \n",
    "- Policy\n",
    "    - Bestimmt wie der Algorithmus lernt\n",
    "    - https://stable-baselines.readthedocs.io/en/master/modules/policies.html\n",
    " \n",
    "- (Optional) DummyVecEnv\n",
    "    - Simulation mehrere Umgebungen und Agenten\n",
    "    - Nützlich um Zeit zu sparen, kostet mehr Resourcen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf1e034-4a95-4ed2-85fb-5a0047c53795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3efce14c-c1c6-4110-ba7f-249abd96e5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)  # Mlp: Multilayer Perceptron -> 2x 64 Dense Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39115681-56bf-4246-99f1-f6be668e9ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 849  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 527         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008665776 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00215    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.64        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 53.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 455         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008519217 |\n",
      "|    clip_fraction        | 0.0654      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 41.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 442          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077282935 |\n",
      "|    clip_fraction        | 0.0827       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.635       |\n",
      "|    explained_variance   | 0.246        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.2         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0188      |\n",
      "|    value_loss           | 53.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 421         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009275302 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 65.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 415         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009954036 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 61.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 410         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005170527 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 407         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008367242 |\n",
      "|    clip_fraction        | 0.0935      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.3        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 55          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 405         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006540277 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.36        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    value_loss           | 46.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 406          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045191934 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.4         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00672     |\n",
      "|    value_loss           | 55.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x223a31ea0e0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9aff9740-a862-4e55-9923-b08d852062b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CartPoleModel.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b542f9-87d7-44f1-bdf9-8ab4cb5c6491",
   "metadata": {},
   "source": [
    "## Modell testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9592825c-cc3c-425a-b910-409042a781f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9ff8a8a-b0f9-405b-874f-ab6bdeba15b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"CartPoleModel.zip\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67d7dff0-223f-4099-8e5f-7080b22df6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchgang 0: Score: 500.0\n",
      "Durchgang 1: Score: 500.0\n",
      "Durchgang 2: Score: 500.0\n",
      "Durchgang 3: Score: 500.0\n",
      "Durchgang 4: Score: 500.0\n",
      "Durchgang 5: Score: 500.0\n",
      "Durchgang 6: Score: 500.0\n",
      "Durchgang 7: Score: 500.0\n",
      "Durchgang 8: Score: 500.0\n",
      "Durchgang 9: Score: 500.0\n",
      "Durchgang 10: Score: 500.0\n",
      "Durchgang 11: Score: 500.0\n",
      "Durchgang 12: Score: 500.0\n",
      "Durchgang 13: Score: 500.0\n",
      "Durchgang 14: Score: 500.0\n",
      "Durchgang 15: Score: 500.0\n",
      "Durchgang 16: Score: 500.0\n",
      "Durchgang 17: Score: 500.0\n",
      "Durchgang 18: Score: 500.0\n",
      "Durchgang 19: Score: 500.0\n"
     ]
    }
   ],
   "source": [
    "durchgaenge = 20\n",
    "for x in range(durchgaenge):\n",
    "    state = env.reset()  # Gibt den Startstate zurück\n",
    "    score = 0  # Score sollte 200 erreichen\n",
    "\n",
    "    while True:\n",
    "        # env.render()\n",
    "        if len(state) == 2:\n",
    "            action = model.predict(state[0])  # Random Action auswählen\n",
    "        else:\n",
    "            action = model.predict(state)\n",
    "        state, reward, done, term, info = env.step(action[0])  # Action an die Umgebung weitergeben\n",
    "        score += reward\n",
    "        if done or term:\n",
    "            break\n",
    "\n",
    "    print(f\"Durchgang {x}: Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a86852-c338-4800-ba65-bc0f9cb4a3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68575bb4-7d5f-459a-8e7e-1da84061f9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
